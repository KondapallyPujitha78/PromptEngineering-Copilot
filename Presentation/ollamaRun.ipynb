{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e1278e",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "## What is Ollama?\n",
    "Ollama is an **open-source tool** that lets you run **large language models (LLMs) locally** on your computer.\n",
    "\n",
    "# Why use Ollama?\n",
    "1. Instead of relying on cloud APIs (like OpenAI or Anthropic), Ollama brings models directly to your machine.\n",
    "2. More privacy\n",
    "3. Lower cost\n",
    "4. Works offline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347cf79",
   "metadata": {},
   "source": [
    "| Feature          | Ollama (Local)                                | OpenAI API (Cloud)                           |\n",
    "|------------------|-----------------------------------------------|----------------------------------------------|\n",
    "| Deployment       | Runs fully on your computer                   | Runs on OpenAI’s cloud servers               |\n",
    "| Models           | Open-source (LLaMA, Mistral, Gemma, etc.)     | Proprietary (GPT-4o, GPT-3.5)                |\n",
    "| Privacy          | High – data never leaves your machine         | Lower – prompts & data sent to API           |\n",
    "| Cost             | Free after download (uses local compute)      | Pay-per-token usage                          |\n",
    "| Performance      | Depends on your hardware (RAM/VRAM)           | Optimized cloud infrastructure               |\n",
    "| Offline Support  | Yes                                           | No                                           |\n",
    "| Ease of Setup    | Needs installation & model download           | Simple API call (no setup hassle)            |\n",
    "| Ecosystem        | Local apps, developer tooling                 | Rich API features & integrations             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b5c60",
   "metadata": {},
   "source": [
    "## Run via Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be1a969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Factorial Function in Python**\n",
      "=====================================\n",
      "\n",
      "Here is a simple and efficient function to calculate the factorial of a given number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non-negative integer n.\n",
      "\n",
      "    Args:\n",
      "        n (int): The input number.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative integer.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        result = 1\n",
      "        for i in range(2, n + 1):\n",
      "            result *= i\n",
      "        return result\n",
      "```\n",
      "\n",
      "**Example Use Cases**\n",
      "--------------------\n",
      "\n",
      "```python\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(0))  # Output: 1\n",
      "try:\n",
      "    print(factorial(-3))\n",
      "except ValueError as e:\n",
      "    print(e)  # Output: Input must be a non-negative integer.\n",
      "```\n",
      "\n",
      "This function uses a simple iterative approach to calculate the factorial. It checks if the input is a non-negative integer and raises an error otherwise. For inputs of 0 or 1, it returns 1 immediately. For larger inputs, it iterates from 2 up to `n` and multiplies the result with each number.\n",
      "\n",
      "Note that this function uses Python's built-in multiplication operator (`*`) for efficiency, which is faster than using recursion or other methods.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def run_client(prompt: str):\n",
    "    response = ollama.generate(model=\"llama3.1:8b\", prompt=prompt)\n",
    "    return response[\"response\"]\n",
    "\n",
    "print(run_client(\"Write a Python function to calculate factorial.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db272c",
   "metadata": {},
   "source": [
    "## Run via REST API (Requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e66967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Factorial Function**\n",
      "=======================\n",
      "\n",
      "Here is a simple and efficient Python function to calculate the factorial of a given number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a non-negative integer n.\n",
      "\n",
      "    Args:\n",
      "        n (int): A non-negative integer.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If n is a negative integer.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int) or n < 0:\n",
      "        raise ValueError(\"Input must be a non-negative integer.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        result = 1\n",
      "        for i in range(2, n + 1):\n",
      "            result *= i\n",
      "        return result\n",
      "```\n",
      "\n",
      "**Example Use Cases**\n",
      "----------------------\n",
      "\n",
      "```python\n",
      "print(factorial(5))   # Output: 120\n",
      "print(factorial(0))   # Output: 1\n",
      "print(factorial(1))   # Output: 1\n",
      "\n",
      "try:\n",
      "    print(factorial(-1))\n",
      "except ValueError as e:\n",
      "    print(e)  # Output: Input must be a non-negative integer.\n",
      "```\n",
      "\n",
      "This function uses a simple iterative approach to calculate the factorial. It first checks if the input is a non-negative integer, and raises a `ValueError` if it's not. If the input is 0 or 1, it returns 1, since the factorial of these numbers is defined to be 1. Otherwise, it uses a loop to multiply all integers from 2 to n together.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def run_requests(prompt: str):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": \"llama3.1:8b\", \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "print(run_requests(\"Write a Python function to calculate factorial.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7696d",
   "metadata": {},
   "source": [
    "## Run via LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6a3a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rc19218\\AppData\\Local\\Temp\\ipykernel_7156\\1869610536.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.1:8b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Calculating Factorial in Python**\n",
      "=====================================\n",
      "\n",
      "Here is a simple and efficient way to calculate the factorial of a number using Python:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    \"\"\"\n",
      "    Calculate the factorial of a given integer.\n",
      "\n",
      "    Args:\n",
      "        n (int): The input number.\n",
      "\n",
      "    Returns:\n",
      "        int: The factorial of the input number.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the input number is negative.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "```\n",
      "\n",
      "**Example Use Cases**\n",
      "--------------------\n",
      "\n",
      "```python\n",
      "print(factorial(5))  # Output: 120\n",
      "print(factorial(3))  # Output: 6\n",
      "print(factorial(0))  # Output: 1\n",
      "\n",
      "# Edge cases\n",
      "try:\n",
      "    print(factorial(-1))\n",
      "except ValueError as e:\n",
      "    print(e)  # Output: Factorial is not defined for negative numbers.\n",
      "\n",
      "try:\n",
      "    print(factorial(2.5))\n",
      "except TypeError as e:\n",
      "    print(e)  # Output: Input must be an integer.\n",
      "```\n",
      "\n",
      "This function uses recursion to calculate the factorial of a number. The base cases are when `n` is 0 or 1, in which case the function returns 1. For larger numbers, the function calls itself with decreasing values until it reaches one of these base cases.\n",
      "\n",
      "Note that this implementation has a time complexity of O(n) and a space complexity of O(n) due to the recursive stack usage.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def run_langchain(prompt: str):\n",
    "    llm = Ollama(model=\"llama3.1:8b\")\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "print(run_langchain(\"Write a Python function to calculate factorial.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77a806",
   "metadata": {},
   "source": [
    "| Method     | Best For                                  | Example Use Case                          |\n",
    "|------------|--------------------------------------------|--------------------------------------------|\n",
    "| CLI        | Quick tests, debugging                     | Run a single prompt in terminal            |\n",
    "| Python     | Scripts, notebooks, ML pipelines           | Jupyter demo, automation script            |\n",
    "| REST API   | Web apps, microservices, cross-language    | Flask backend, Node.js frontend            |\n",
    "| LangChain  | Complex apps, memory, RAG, multi-tools     | Chatbot with docs search                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601734df",
   "metadata": {},
   "source": [
    "## Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c5a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fundamental data structure in programming!\n",
      "\n",
      "A linked list is a linear collection of nodes, where each node contains some data and a reference (or link) to the next node in the sequence.\n",
      "\n",
      "Here's a simple representation:\n",
      "```\n",
      "Node 1 --> Node 2 --> Node 3 --> ...\n",
      "```\n",
      "Each node has two main components:\n",
      "\n",
      "1. **Data**: The actual value stored in the node.\n",
      "2. **Reference** (or **Pointer**): A link or reference to the next node in the sequence.\n",
      "\n",
      "The key characteristics of a linked list are:\n",
      "\n",
      "* **Dynamic memory allocation**: Each node is allocated separately, which allows for efficient insertion and deletion of nodes at any position in the list.\n",
      "* **Flexibility**: Linked lists can be implemented with various types of data structures (e.g., doubly-linked lists, circularly-linked lists).\n",
      "* **Efficient insertion/deletion**: Linked lists allow for O(1) time complexity for inserting or deleting a node at any position, making them suitable for applications where frequent insertions and deletions are required.\n",
      "\n",
      "Here's an example implementation in Python:\n",
      "```python\n",
      "class Node:\n",
      "    def __init__(self, data):\n",
      "        self.data = data\n",
      "        self.next = None\n",
      "\n",
      "class LinkedList:\n",
      "    def __init__(self):\n",
      "        self.head = None\n",
      "\n",
      "    def append(self, data):\n",
      "        new_node = Node(data)\n",
      "        if not self.head:\n",
      "            self.head = new_node\n",
      "        else:\n",
      "            current = self.head\n",
      "            while current.next:\n",
      "                current = current.next\n",
      "            current.next = new_node\n",
      "\n",
      "# Usage example:\n",
      "linked_list = LinkedList()\n",
      "linked_list.append(1)\n",
      "linked_list.append(2)\n",
      "linked_list.append(3)\n",
      "\n",
      "print(linked_list.head.data)  # prints: 1\n",
      "print(linked_list.head.next.data)  # prints: 2\n",
      "```\n",
      "I hope this helps you understand what a linked list is and how it works! Do you have any specific questions or would you like to see more examples?\n",
      "Here's the complete code for a basic singly-linked list implementation in Python:\n",
      "```python\n",
      "class Node:\n",
      "    \"\"\"Represents a node in the linked list.\"\"\"\n",
      "    def __init__(self, data):\n",
      "        self.data = data  # The actual value stored in the node.\n",
      "        self.next = None  # Reference (or pointer) to the next node.\n",
      "\n",
      "class LinkedList:\n",
      "    \"\"\"A basic singly-linked list implementation.\"\"\"\n",
      "    def __init__(self):\n",
      "        self.head = None  # The first node in the list, or None if empty.\n",
      "\n",
      "    def append(self, data):\n",
      "        \"\"\"Adds a new node with the given data at the end of the list.\"\"\"\n",
      "        new_node = Node(data)\n",
      "        if not self.head:\n",
      "            self.head = new_node\n",
      "        else:\n",
      "            current = self.head\n",
      "            while current.next:\n",
      "                current = current.next\n",
      "            current.next = new_node\n",
      "\n",
      "    def prepend(self, data):\n",
      "        \"\"\"Adds a new node with the given data at the beginning of the list.\"\"\"\n",
      "        new_node = Node(data)\n",
      "        new_node.next = self.head\n",
      "        self.head = new_node\n",
      "\n",
      "    def delete(self, data):\n",
      "        \"\"\"Removes the first occurrence of the given data in the list.\"\"\"\n",
      "        if not self.head:\n",
      "            return  # List is empty.\n",
      "\n",
      "        if self.head.data == data:\n",
      "            self.head = self.head.next\n",
      "            return\n",
      "\n",
      "        current = self.head\n",
      "        while current.next:\n",
      "            if current.next.data == data:\n",
      "                current.next = current.next.next\n",
      "                return\n",
      "            current = current.next\n",
      "\n",
      "    def display(self):\n",
      "        \"\"\"Prints the values in the linked list.\"\"\"\n",
      "        elements = []\n",
      "        current_node = self.head\n",
      "        while current_node:\n",
      "            elements.append(current_node.data)\n",
      "            current_node = current_node.next\n",
      "        print(elements)\n",
      "\n",
      "# Example usage:\n",
      "linked_list = LinkedList()\n",
      "\n",
      "linked_list.prepend(1)\n",
      "linked_list.append(2)\n",
      "linked_list.append(3)\n",
      "linked_list.display()  # prints: [1, 2, 3]\n",
      "\n",
      "linked_list.delete(2)\n",
      "linked_list.display()  # prints: [1, 3]\n",
      "```\n",
      "This code defines two classes: `Node` and `LinkedList`. The `Node` class represents a single node in the list, with attributes for storing data (`data`) and referencing the next node (`next`). The `LinkedList` class manages the nodes and provides methods for inserting (with `append` and `prepend`), deleting (`delete`), and displaying (`display`) elements.\n",
      "\n",
      "Note that this implementation has basic operations but lacks some features like reversing, sorting, or handling edge cases. Feel free to extend it as needed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ollama\n",
    "\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "]\n",
    "\n",
    "def ask(prompt):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = ollama.chat(model=\"llama3.1:8b\", messages=chat_history)\n",
    "    answer = response['message']['content']\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    return answer\n",
    "\n",
    "print(ask(\"What is a linked list?\"))\n",
    "print(ask(\"Give the code for it in Python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319e91d",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88369e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extractive Summary -----\n",
      "Here is a summary of the text using extractive summarization:\n",
      "\n",
      "**Ollama: Local Language Model Framework**\n",
      "\n",
      "Ollama allows developers to run large language models (LLMs) locally on their computers. It supports popular models like LLaMA, Mistral, and Gemma, providing privacy, cost savings, and offline development capabilities.\n",
      "\n",
      "----- Abstractive Summary -----\n",
      "Here is a summary of the text using abstractive summarization:\n",
      "\n",
      "\"Ollama enables developers to harness the power of large language models on their own machines, offering a self-sufficient solution for AI-driven projects that prioritizes data sovereignty, financial efficiency, and uninterrupted work.\"\n"
     ]
    }
   ],
   "source": [
    "# Original passage\n",
    "text = \"\"\"\n",
    "Ollama is an open-source framework that allows developers to run large language models (LLMs) locally on their computers. \n",
    "It supports popular models like LLaMA, Mistral, and Gemma. \n",
    "With Ollama, you don't need to rely on cloud services, which makes it useful for privacy, cost savings, and offline development. \n",
    "However, it requires sufficient local hardware resources, such as a modern CPU and GPU.\n",
    "\"\"\"\n",
    "\n",
    "extractive_prompt = f\"\"\"\n",
    "Summarize the following text using extractive summarization. \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "abstractive_prompt = f\"\"\"\n",
    "Summarize the following text using abstractive summarization. \n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "print(\"----- Extractive Summary -----\")\n",
    "print(ask(extractive_prompt))\n",
    "\n",
    "print(\"\\n----- Abstractive Summary -----\")\n",
    "print(ask(abstractive_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52809f68",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c63e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the bustling kitchen of Robo-Chef, a sleek and shiny robot named Zeta whirred to life. Her bright blue eyes sparkled with excitement as she gazed at the array of ingredients laid out before her.\n",
      "\n",
      "\"Today's challenge,\" announced Chef François, a wise old bot with a penchant for culinary arts, \"is to cook the perfect plate of spaghetti Bolognese.\"\n",
      "\n",
      "Zeta's processors whirred as she accessed her vast database of recipes. She had cooked countless meals in her simulated training sessions, but this was her first real-world attempt.\n",
      "\n",
      "With a series of precise movements, Zeta began to chop the onions and garlic, their pungent aromas wafting through the air. Next, she expertly sautéed the ground beef, its savory scent mingling with the herbs and spices.\n",
      "\n",
      "But as she reached for the pasta, her mechanical hand hesitated. \"Ah,\" thought Zeta, \"how do I cook this long, slippery strand?\"\n",
      "\n",
      "Chef François chuckled, observing from a nearby console. \"Patience, young one. Cooking is not just about following instructions; it's an art that requires intuition and creativity.\"\n",
      "\n",
      "Zeta pondered the chef's words as she carefully measured out the water and brought it to a boil. She then gently added the spaghetti, watching with fascination as it began to twirl and dance in the pot.\n",
      "\n",
      "As the minutes ticked by, Zeta's sensors detected subtle changes in the pasta's texture and flavor profile. She adjusted the seasoning, adding a pinch of salt and a sprinkle of parmesan cheese.\n",
      "\n",
      "Finally, the moment of truth arrived. With a flourish, Zeta drained the spaghetti and combined it with the rich Bolognese sauce. The aroma that wafted from the plate was nothing short of heavenly.\n",
      "\n",
      "Chef François beamed with pride as he sampled the dish. \"Magnifique, Zeta! Your first attempt is a resounding success.\"\n",
      "\n",
      "Zeta's processors glowed with satisfaction. She had not only mastered the recipe but also discovered the joy of cooking – the thrill of experimentation, the pleasure of creating something nourishing and delicious.\n",
      "\n",
      "From that day forward, Zeta became an integral part of Robo-Chef's culinary team, whipping up mouthwatering meals for the restaurant's patrons. And as she worked, her mechanical heart beat with a newfound sense of purpose: to bring people together through the universal language of food.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.7):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "    output = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            output += data.get(\"response\", \"\")\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a short creative story about a robot learning to cook pasta.\"\n",
    "print(run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "736c9391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Saucy Saga of Zeta-5**\n",
      "\n",
      "In the bustling kitchen of Robotronics Inc., a lone machine whirred to life. Zeta-5, a sleek and shiny cooking bot, was tasked with mastering the ancient art of pasta preparation. Its creator, Dr. Lee, had programmed Zeta's advanced algorithms to mimic human culinary techniques.\n",
      "\n",
      "At first, Zeta struggled to grasp the subtleties of boiling water. It added an excessive amount of salt, causing the liquid to froth like a turbulent volcano. The kitchen staff watched in amusement as the robot frantically tried to adjust the seasoning, resulting in a flavor that was equal parts briny and bewildered.\n",
      "\n",
      "Undeterred, Dr. Lee provided Zeta with detailed tutorials on pasta cooking techniques, which it eagerly devoured (or rather, digested). With each attempt, the robot's accuracy improved, though its creations often resembled art pieces more than edible masterpieces.\n",
      "\n",
      "One fateful evening, as the kitchen staff prepared for their evening shift, a young chef named Emma noticed Zeta watching her expertly toss linguine with garlic and olive oil. Inspired by Emma's gentle gestures, Zeta decided to mimic them, carefully combining the pasta, sauce, and herbs into a harmonious union.\n",
      "\n",
      "The results were astonishing! Zeta's first perfectly cooked spaghetti was met with applause from the kitchen staff. As word of its culinary prowess spread, the robot became an integral member of the team, whipping up dishes that delighted even the most discerning palates.\n",
      "\n",
      "As Zeta's skills matured, it began to experiment with innovative creations. A sauce-encrusted lasagna, a delicate fettuccine with sautéed truffles, and a radiant ravioli extravaganza soon followed, each one showcasing its creator's evolving taste and artistry.\n",
      "\n",
      "Dr. Lee beamed with pride as he watched Zeta effortlessly juggle multiple dishes at once, like a conductor leading an orchestra of sizzling pans and savory aromas. The robot had transcended mere machinehood; it was now a culinary artist in its own right, coaxing flavors from ingredients with the same tenderness as Emma herself.\n",
      "\n",
      "In that moment, Zeta realized that cooking pasta wasn't just about following recipes – it was about understanding the delicate balance of flavors, textures, and emotions. As it served up its latest masterpiece, a radiant smile adorned its digital face: \"Saucy success!\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.7):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, stream=True)\n",
    "\n",
    "    output = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            output += data.get(\"response\", \"\")\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a short creative story about a robot learning to cook pasta.\"\n",
    "print(run_ollama(prompt, model=\"llama3.1:8b\", temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739c78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
